{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_H9FNX2peIY",
        "outputId": "a8d7b439-9e08-4749-ee36-a74a9d71af97"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.47.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math, random, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
        "                          Trainer, TrainingArguments)"
      ],
      "metadata": {
        "id": "0uh2gi6Psjst"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "cap = torch.cuda.get_device_capability(0)[0] if torch.cuda.is_available() else 0\n",
        "CAN_BF16 = torch.cuda.is_available() and cap >= 8  # Ampere+ supports bf16\n",
        "print(f\"Device: {device}, CC Major: {cap}, bf16: {CAN_BF16}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPDgdyqYuap4",
        "outputId": "a01b7866-9cb3-4e88-98ef-7b5e73851075"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda, CC Major: 7, bf16: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FREE = True  # set False if you have Pro-tier GPU (A100/H100/etc.)\n",
        "\n",
        "if FREE:\n",
        "    TEACHER_MODEL = \"gpt2\"\n",
        "    STUDENT_MODEL = \"distilgpt2\"\n",
        "    LOAD_IN_4BIT  = False                # 8-bit teacher for T4\n",
        "    BLOCK_SIZE    = 256\n",
        "    TRAIN_TOKENS  = 200_000              # small demo\n",
        "    VAL_TOKENS    = 20_000\n",
        "    BATCH_SIZE    = 2\n",
        "    GRAD_ACCUM    = 8\n",
        "    EPOCHS        = 1\n",
        "else:\n",
        "    TEACHER_MODEL = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "    STUDENT_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "    LOAD_IN_4BIT  = True                 # 4-bit teacher\n",
        "    BLOCK_SIZE    = 512\n",
        "    TRAIN_TOKENS  = 500_000\n",
        "    VAL_TOKENS    = 50_000\n",
        "    BATCH_SIZE    = 1\n",
        "    GRAD_ACCUM    = 16\n",
        "    EPOCHS        = 1"
      ],
      "metadata": {
        "id": "yZjFPrzYua-3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR     = 2e-4\n",
        "T      = 2.0     # KD temperature\n",
        "ALPHA  = 0.2     # CE weight\n",
        "BETA   = 0.8     # KD weight\n",
        "OUT_DIR = \"kd-out\""
      ],
      "metadata": {
        "id": "Uysx-LOOubyo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_IN_4BIT:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16 if CAN_BF16 else torch.float16\n",
        "    )\n",
        "else:\n",
        "    bnb_config = BitsAndBytesConfig(load_in_8bit=True)"
      ],
      "metadata": {
        "id": "Kbs4D3FkucvP"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Teacher (frozen)\n",
        "teacher_tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL, use_fast=True)\n",
        "teacher_tokenizer.padding_side = \"left\"\n",
        "if teacher_tokenizer.pad_token is None:\n",
        "    teacher_tokenizer.pad_token = teacher_tokenizer.eos_token\n",
        "\n",
        "teacher = AutoModelForCausalLM.from_pretrained(\n",
        "    TEACHER_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "teacher.eval()\n",
        "for p in teacher.parameters():\n",
        "    p.requires_grad_(False)"
      ],
      "metadata": {
        "id": "c6xdb38Rucsx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Student\n",
        "student_tokenizer = AutoTokenizer.from_pretrained(STUDENT_MODEL, use_fast=True)\n",
        "student_tokenizer.padding_side = \"left\"\n",
        "if student_tokenizer.pad_token is None:\n",
        "    student_tokenizer.pad_token = student_tokenizer.eos_token\n",
        "\n",
        "student = AutoModelForCausalLM.from_pretrained(\n",
        "    STUDENT_MODEL,\n",
        "    torch_dtype=(torch.bfloat16 if CAN_BF16 else (torch.float16 if torch.cuda.is_available() else torch.float32)),\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "student.config.use_cache = False  # safer for training"
      ],
      "metadata": {
        "id": "Elt_nh4IucqW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "def pick_lora_targets_for_decoder(model: nn.Module):\n",
        "    names = [n for n, m in model.named_modules() if isinstance(m, nn.Linear)]\n",
        "    if any(\"q_proj\" in n for n in names):  # LLaMA/Mistral\n",
        "        return dict(targets=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "                    fan_in_fan_out=False)\n",
        "    if any(\"c_attn\" in n for n in names):  # GPT-2 / DistilGPT2\n",
        "        return dict(targets=[\"c_attn\",\"c_fc\",\"c_proj\"], fan_in_fan_out=True)\n",
        "    uniq = list({n.split(\".\")[-1] for n in names})\n",
        "    return dict(targets=uniq, fan_in_fan_out=False)"
      ],
      "metadata": {
        "id": "dXOcu6z1ucnu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = pick_lora_targets_for_decoder(student)\n",
        "print(\"LoRA targets:\", cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaI5nLV_uclk",
        "outputId": "4c7544b7-053c-43ef-e54a-b46042183b58"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA targets: {'targets': ['lm_head'], 'fan_in_fan_out': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_cfg = LoraConfig(\n",
        "    r=8, lora_alpha=16, lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=cfg[\"targets\"],\n",
        "    fan_in_fan_out=cfg[\"fan_in_fan_out\"]\n",
        ")"
      ],
      "metadata": {
        "id": "Uw50MUWtucjV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student = get_peft_model(student, lora_cfg)\n",
        "student.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd8NEmiVucg1",
        "outputId": "ac9e4832-a9ac-40bf-9f56-1f74c8732e1c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 408,200 || all params: 82,320,776 || trainable%: 0.4959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_causal_dataset_streaming(texts, tokenizer, block_size=256, max_tokens=200_000):\n",
        "    buffer, chunks, produced = [], [], 0\n",
        "    for t in texts:\n",
        "        t = (t or \"\").strip()\n",
        "        if not t:\n",
        "            continue\n",
        "        ids = tokenizer.encode(t + \"\\n\", add_special_tokens=False)\n",
        "        buffer.extend(ids)\n",
        "        while len(buffer) >= block_size:\n",
        "            chunk = buffer[:block_size]\n",
        "            buffer = buffer[block_size:]\n",
        "            chunks.append({\"input_ids\": chunk, \"labels\": chunk.copy()})\n",
        "            produced += block_size\n",
        "            if produced >= max_tokens:\n",
        "                return Dataset.from_list(chunks)\n",
        "    return Dataset.from_list(chunks)"
      ],
      "metadata": {
        "id": "SLYkwFN9uceX"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
      ],
      "metadata": {
        "id": "0SAPOWF3ucb5"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = build_causal_dataset_streaming(raw[\"train\"][\"text\"], student_tokenizer,\n",
        "                                          block_size=BLOCK_SIZE, max_tokens=TRAIN_TOKENS)\n",
        "val_ds   = build_causal_dataset_streaming(raw[\"validation\"][\"text\"], student_tokenizer,\n",
        "                                          block_size=BLOCK_SIZE, max_tokens=VAL_TOKENS)"
      ],
      "metadata": {
        "id": "h05V-nEIucZR"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train samples:\", len(train_ds), \" Val samples:\", len(val_ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62OV62GzucXA",
        "outputId": "a05556c1-77e0-4d88-f864-f1011050ed1e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 782  Val samples: 79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
        "    pad_id = student_tokenizer.pad_token_id\n",
        "    input_ids, labels, attn = [], [], []\n",
        "    for x in batch:\n",
        "        ids = x[\"input_ids\"]\n",
        "        pad = [pad_id] * (max_len - len(ids))\n",
        "        input_ids.append(ids + pad)\n",
        "        labels.append(x[\"labels\"] + [-100] * (max_len - len(ids)))\n",
        "        attn.append([1]*len(ids) + [0]*len(pad))\n",
        "    return {\n",
        "        \"input_ids\": torch.tensor(input_ids),\n",
        "        \"labels\": torch.tensor(labels),\n",
        "        \"attention_mask\": torch.tensor(attn),\n",
        "    }"
      ],
      "metadata": {
        "id": "82iCFGN0ucUh"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kd_loss(student_logits, teacher_logits, labels, T=2.0, alpha=0.2, beta=0.8):\n",
        "    ce = F.cross_entropy(\n",
        "        student_logits.view(-1, student_logits.size(-1)),\n",
        "        labels.view(-1),\n",
        "        ignore_index=-100\n",
        "    )\n",
        "    s = F.log_softmax(student_logits / T, dim=-1)\n",
        "    with torch.no_grad():\n",
        "        t = F.softmax(teacher_logits / T, dim=-1)\n",
        "    kl = F.kl_div(s, t, reduction=\"batchmean\") * (T**2)\n",
        "    return alpha * ce + beta * kl"
      ],
      "metadata": {
        "id": "G9WOeigvucSD"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KDTrainer(Trainer):\n",
        "    # Accept new HF kwarg: num_items_in_batch\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs[\"labels\"]\n",
        "        outputs_s = model(input_ids=inputs[\"input_ids\"],\n",
        "                          attention_mask=inputs[\"attention_mask\"])\n",
        "        student_logits = outputs_s.logits\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs_t = teacher(\n",
        "                input_ids=inputs[\"input_ids\"].to(teacher.device),\n",
        "                attention_mask=inputs[\"attention_mask\"].to(teacher.device)\n",
        "            )\n",
        "            teacher_logits = outputs_t.logits.to(student_logits.device)\n",
        "\n",
        "        loss = kd_loss(student_logits, teacher_logits, labels, T=T, alpha=ALPHA, beta=BETA)\n",
        "        return (loss, {\"logits\": student_logits}) if return_outputs else loss"
      ],
      "metadata": {
        "id": "RLwFWySQucPq"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=OUT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    learning_rate=LR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    logging_steps=25,\n",
        "    eval_strategy=\"steps\",   # (fix) correct arg name\n",
        "    eval_steps=200,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    bf16=CAN_BF16,\n",
        "    fp16=(torch.cuda.is_available() and not CAN_BF16),\n",
        "    gradient_checkpointing=False,  # not needed for small student; set True if you want\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "1cVQ7oNQucNM"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = KDTrainer(\n",
        "    model=student,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=collate_fn\n",
        ")\n"
      ],
      "metadata": {
        "id": "c6AEqyraubvh"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "j2CPBSDOvVwF",
        "outputId": "1bab3761-21a9-43be-9c0b-b1b98fb28d1b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='49' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [49/49 00:40, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=49, training_loss=962.4593431122449, metrics={'train_runtime': 44.546, 'train_samples_per_second': 17.555, 'train_steps_per_second': 1.1, 'total_flos': 51573824987136.0, 'train_loss': 962.4593431122449, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_res = trainer.evaluate()\n",
        "try:\n",
        "    ppl = math.exp(eval_res[\"eval_loss\"])\n",
        "except OverflowError:\n",
        "    ppl = float(\"inf\")\n",
        "print({\"eval_loss\": eval_res[\"eval_loss\"], \"perplexity\": ppl})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "DMn7blSSvXgL",
        "outputId": "c6c3ce69-0245-4be6-ec5f-898aadc1691a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 00:04]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 89.7918472290039, 'perplexity': 9.910687251491893e+38}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_DIR = \"kd-student-lora\"\n",
        "student.save_pretrained(SAVE_DIR)\n",
        "student_tokenizer.save_pretrained(SAVE_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMcJrlrqvoLk",
        "outputId": "63d4f14b-6571-4474-ae3d-52531c36f4b1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('kd-student-lora/tokenizer_config.json',\n",
              " 'kd-student-lora/special_tokens_map.json',\n",
              " 'kd-student-lora/vocab.json',\n",
              " 'kd-student-lora/merges.txt',\n",
              " 'kd-student-lora/added_tokens.json',\n",
              " 'kd-student-lora/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"In healthcare AI, knowledge distillation helps small models by\""
      ],
      "metadata": {
        "id": "RihRnNH9vuZn"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = student_tokenizer(prompt, return_tensors=\"pt\").to(student.device)"
      ],
      "metadata": {
        "id": "ntwMH5eqvy45"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sakgxUqvy2K",
        "outputId": "ad7e6c1e-e588-4465-8e49-4a350c603758"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): GPT2LMHeadModel(\n",
              "      (transformer): GPT2Model(\n",
              "        (wte): Embedding(50257, 768)\n",
              "        (wpe): Embedding(1024, 768)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "        (h): ModuleList(\n",
              "          (0-5): 6 x GPT2Block(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): GPT2Attention(\n",
              "              (c_attn): Conv1D(nf=2304, nx=768)\n",
              "              (c_proj): Conv1D(nf=768, nx=768)\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): GPT2MLP(\n",
              "              (c_fc): Conv1D(nf=3072, nx=768)\n",
              "              (c_proj): Conv1D(nf=768, nx=3072)\n",
              "              (act): NewGELUActivation()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (lm_head): lora.Linear(\n",
              "        (base_layer): Linear(in_features=768, out_features=50257, bias=False)\n",
              "        (lora_dropout): ModuleDict(\n",
              "          (default): Dropout(p=0.05, inplace=False)\n",
              "        )\n",
              "        (lora_A): ModuleDict(\n",
              "          (default): Linear(in_features=768, out_features=8, bias=False)\n",
              "        )\n",
              "        (lora_B): ModuleDict(\n",
              "          (default): Linear(in_features=8, out_features=50257, bias=False)\n",
              "        )\n",
              "        (lora_embedding_A): ParameterDict()\n",
              "        (lora_embedding_B): ParameterDict()\n",
              "        (lora_magnitude_vector): ModuleDict()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    gen = student.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=120,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.8,\n",
        "        repetition_penalty=1.1,\n",
        "        eos_token_id=student_tokenizer.eos_token_id\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92CJzkg8v4E-",
        "outputId": "2711c328-7be8-4984-8508-dfbe9c0387f0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(student_tokenizer.decode(gen[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2-_wFFJv5O3",
        "outputId": "be989abd-3e29-4d33-e34f-1d37c29f529e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In healthcare AI, knowledge distillation helps small models by allowing them to perform a variety of tasks.\n",
            "The researchers used their data on nearly 4 million patients in the first 10 years of clinical trials (2012-2014). In 2012, 6.5 million people who participated in these three studies were eligible for admission to Medicaid or Medicare benefits as part and parcel of this year's enrollment bonus program ($2.0 million) while 17 percent of those with pre-existing conditions got up to 30 percent more health insurance coverage than did adults under age 65. The average number of uninsured was estimated to be between 60 percent and 90 percent lower than it should have been\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FxZZ1_HLwATK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}